{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBA DSA USP/ESALQ 2024.1 Time 2\n",
    "## Desafio House Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza e ajuste dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura dos dados do arquivo para treino (\"train.csv\") e armazenando no objeto 'dados_brutos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_brutos = pd.read_csv(\"train.csv\")\n",
    "\n",
    "print(\"Quantidade de linhas e colunas\")\n",
    "print(dados_brutos.shape)\n",
    "\n",
    "print(\"\\nTipos de objetos do DataFrame\")\n",
    "print(dados_brutos.dtypes)\n",
    "\n",
    "print(\"\\nInformações sobre o banco de dados (coluna, contador de nulos, e tipo de informação)\")\n",
    "dados_brutos.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O banco de dados possui uma série de colunas (variáveis) que possuem valores nulos, algumas em maior quantidade e outras em menor quantidade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos = dados_brutos.isnull().sum()\n",
    "nulos_ordenados = nulos.sort_values(ascending=False)\n",
    "\n",
    "print(\"Quantidade de valores nulos por coluna:\")\n",
    "print(nulos_ordenados.head(20))\n",
    "print(\"Como o último valor já é zero, depois desse todos são zerados mesmo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como são valores absolutos, é mais fácil excluir do banco de dados as variáveis que possuam valores nulos acima de um determinado percentual do total (eu usei 15% como delimitador), salvando no objeto 'dados_filtrados'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos_percentuais = nulos/len(dados_brutos.index)\n",
    "var_excluir = dados_brutos.columns[nulos_percentuais > 0.15]\n",
    "dados_filtrados = dados_brutos.drop(var_excluir, axis = 1)\n",
    "\n",
    "print(\"Quantidade de linhas e colunas depois de ajustado\")\n",
    "print(dados_filtrados.shape)\n",
    "\n",
    "print(\"\\nTipos de objetos do DataFrame\")\n",
    "print(dados_filtrados.dtypes)\n",
    "\n",
    "print(\"\\nInformações sobre o banco de dados (coluna, contador de nulos, e tipo de informação)\")\n",
    "dados_filtrados.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso feito, existem variáveis que são 'object' (não-numérico, então qualitativas ou categóricas) e variáveis que são \"intXX\" (portanto, numéricas). Por questão de facilidade e considerando o tempo disponível, por enquanto vou desconsiderar as variáveis 'quali' e vou utilizar apenas as 'quanti'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_quali = dados_filtrados.columns[dados_filtrados.dtypes == \"object\"]\n",
    "variaveis_quanti = dados_filtrados.drop(colunas_quali, axis = 1)\n",
    "print(\"AS VARIÁVEIS NUMÉRICAS ('QUANTI') ENTÃO SÃO 37:\")\n",
    "print(variaveis_quanti.columns)\n",
    "print(\"\\n\")\n",
    "\n",
    "colunas_quanti = dados_filtrados.columns[dados_filtrados.dtypes != \"object\"]\n",
    "variaveis_quali = dados_filtrados.drop(colunas_quanti, axis = 1)\n",
    "print(\"AS VARIÁVEIS NÃO-NÚMERICAS ('QUALI') FICAM SENDO APENAS 38:\")\n",
    "print(variaveis_quali.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Filtro das variáveis quali e quanti para evitar confusão. Contudo, ainda existem valores nulos no meio dos dados (foram eliminados só as variações com nulos >= 15% do total). Necessário realizar a imputação esses valores, pois nenhuma análise é feita com qualquer valor nulo: quebra o sistema.\n",
    "\n",
    " Link sobre o carregamento das bibliotecas para imputar dados: https://medium.com/@sanjushusanth/missing-value-imputation-techniques-in-python-62aeab65a6a6\n",
    "\n",
    " Por mera facilidade momentânea, serão utilizados os dados de média e mediana para as imputações, ou, ainda seguindo o que alguns consideram 'boas práticas', simplesmente registrando um '-1' no valor imputado. Fiz um banco de dados para cada tipo dessas imputações a fim de testar qual melhor se encaixa nessa simples imputação (sem cálculos próprios de imputação para esse fim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_quanti_namum = variaveis_quanti.fillna(-1)\n",
    "variaveis_quanti_namediana = variaveis_quanti.fillna(variaveis_quanti.median())\n",
    "variaveis_quanti_namedia = variaveis_quanti.fillna(variaveis_quanti.mean())\n",
    "\n",
    "print(\"VERIFICANDO VARIÁVEIS C/ IMPUTAÇÃO '-1' AINDA COM VALORES NULOS...\")\n",
    "print(variaveis_quanti_namum.columns[variaveis_quanti_namum.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\n VERIFICANDO VARIÁVEIS C/ IMPUTAÇÃO 'MÉDIA' AINDA COM VALORES NULOS...\")\n",
    "print(variaveis_quanti_namum.columns[variaveis_quanti_namedia.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\n VERIFICANDO VARIÁVEIS C/ IMPUTAÇÃO 'MEDIANA' AINDA COM VALORES NULOS...\")\n",
    "print(variaveis_quanti_namum.columns[variaveis_quanti_namediana.isnull().sum() > 0])\n",
    "\n",
    "try:\n",
    "    variaveis_quanti_namum.columns[variaveis_quanti_namediana.isnull().sum() > 0] + variaveis_quanti_namum.columns[variaveis_quanti_namedia.isnull().sum() > 0]+ variaveis_quanti_namum.columns[variaveis_quanti_namum.isnull().sum() > 0]\n",
    "    print(\"\\nTUDO CERTO!\")\n",
    "except:\n",
    "    print(\"\\n!!! DEU ALGUM ERRO !!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição da variável dependente (y) e das variáveis independentes (x) de cada um dos bancos de dados anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_namum = variaveis_quanti_namum.drop(\"SalePrice\", axis = 1)\n",
    "x_namedia = variaveis_quanti_namedia.drop(\"SalePrice\", axis = 1)\n",
    "x_namediana = variaveis_quanti_namediana.drop(\"SalePrice\", axis = 1)\n",
    "y = variaveis_quanti.SalePrice # como o 'y' é igual para todos (o preço de venda), então tanto faz a base de dados escolhida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação dos dados para treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas adicionais necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separação do banco de dados entre treino (0,80 ou 80%) e teste (0,20 ou 20%) - proporções arbitrárias, eu que defini assim. Pode-se testar com proporções diferentes. Usa-se o scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Para garantir consistência entre os resultados, utilizado sempre _random_state = 1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para os dados imputados com -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_namum, teste_namum, treino_y, teste_y = train_test_split(x_namum, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "print(\"DADOS TREINO NA COM -1\")\n",
    "print(treino_namum.head(5))\n",
    "\n",
    "print(\"\\nDADOS TESTE NA COM -1\")\n",
    "print(teste_namum.head(5))\n",
    "\n",
    "print(\"\\n DADOS TREINO SalePrice\")\n",
    "print(treino_y.head(5))\n",
    "\n",
    "print(\"\\nDADOS TESTE SalePrice\")\n",
    "print(teste_y.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para os dados imputados com a média:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_namedia, teste_namedia, treino_y, teste_y = train_test_split(x_namedia, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "print(\"DADOS TREINO NA COM MÉDIA\")\n",
    "print(treino_namedia.head(5))\n",
    "\n",
    "print(\"\\nDADOS TESTE NA COM MÉDIA\")\n",
    "print(teste_namedia.head(5))\n",
    "\n",
    "print(\"\\n DADOS TREINO SalePrice\")\n",
    "print(treino_y.head(5))\n",
    "\n",
    "print(\"\\nDADOS TESTE SalePrice\")\n",
    "print(teste_y.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para os dados imputados com a mediana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_namediana, teste_namediana, treino_y, teste_y = train_test_split(x_namediana, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "print(\"DADOS TREINO NA COM MEDIANA\")\n",
    "print(treino_namediana.head(5))\n",
    "\n",
    "print(\"\\nDADOS TESTE NA COM MEDIANA\")\n",
    "print(teste_namediana.head(5))\n",
    "\n",
    "print(\"\\n DADOS TREINO SalePrice\")\n",
    "print(treino_y.head(5))\n",
    "\n",
    "print(\"\\nDADOS TESTE SalePrice\")\n",
    "print(teste_y.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelagem\n",
    "Com os dados já limpos, já separados pelas imputações diferentes para verificação e com as bases para treino e teste, começa-se efetivamente a modelagem através de alguns algoritmos diferentes a fim de testar precisão dos resultados estimados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas adicionais necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # biblioteca para modelagem através de regressão Linear Simples\n",
    "from sklearn import tree # biblioteca para modelagem através de árvore de decisão\n",
    "from sklearn.neighbors import KNeighborsRegressor # biblioteca para modelagem através de KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaN imputado com -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regressão Linear Simples\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_namum, teste_namum, treino_y, teste_y = train_test_split(x_namum, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "namum_rl = LinearRegression().fit(treino_namum, treino_y)\n",
    "y_namum_rl = namum_rl.predict(teste_namum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Árvore de Decisão\n",
    "https://scikit-learn.org/stable/modules/tree.html#regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namum_tree = tree.DecisionTreeRegressor(randomm_state = 1).fit(treino_namum, treino_y)\n",
    "y_namum_tree = namum_tree.predict(teste_namum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K Vizinhos Mais Próximos (KNN - K Nearest Neighbors)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namum_knn = KNeighborsRegressor(n_neighbors = 2).fit(treino_namum, treino_y)\n",
    "y_namum_knn = namum_knn.predict(teste_namum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação dos resultados\n",
    "Realizada através de RMSE (root mean squared error), no caso deste desafio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas adicionais necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análises (pela imputação)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NaN por -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
